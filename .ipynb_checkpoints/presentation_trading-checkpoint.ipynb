{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da3297d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML Application to Quantitative Trading\n",
    "\n",
    "In this project, merge sound financial strategies with advanced computer science skills by developing, from scratch in Python, a manual trading strategy, a custom-built market simulator, and a sophisticated machine learning-based Q-Learner with Dyna optimization. The goal was not to develop a groundbreaking trading strategy, but to demonstrate my proficiency in translating financial strategies into code and developing advanced learners without relying on external libraries. ...and to have fun!\n",
    "\n",
    "- **Market Simulator**: I developed this simulator to emulate realistic trading conditions, enabling effective testing of both manual and AI-driven strategies under practical constraints such as transaction fees and share limits.\n",
    "- **Manual Trading Strategy**: I crafted this strategy using traditional financial indicators, aiming to establish a solid foundation for assessing the enhancements that machine learning techniques could bring.\n",
    "- **Machine Learning Strategy**: My self-developed Q-Learner, enhanced with Dyna functionality, was designed to dynamically adapt its trading decisions, showcasing the potential for uncovering and leveraging complex market patterns.\n",
    "\n",
    "This project further validates my ability to excel in innovative settings, where transforming financial insights into sophisticated algorithmic solutions is crucial.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec0b50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Disclaimer and Copyright Notice\n",
    "\n",
    "All functional implementations presented in this report have been independently developed by me, Franz Adam. If code or methodology is used please reference me accoridngly. I advise current students enrolled in courses that cover similar material to adhere to their academic institution's policies regarding integrity and plagiarism before continuing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99917679",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem Space and Constraints\n",
    "\n",
    "### Market Simulation and Trading Constraints\n",
    "\n",
    "This project necessitates a sophisticated simulation of the trading environment to effectively evaluate and demonstrate the capabilities of both manual and AI-driven trading strategies. Utilizing official stock prices for JPMorgan Chase & Co. (JPM), the simulation covers a training period from **January 1, 2008, to December 31, 2009**, and a testing period **January 1, 2010, to December 31, 2011**. The primary focus is not merely on achieving superior returns but on showcasing my expertise in crafting and implementing a comprehensive market simulation and trading algorithms from scratch.\n",
    "\n",
    "### Financial and Trading Specifications\n",
    "\n",
    "- **Starting Capital**: Each strategy begins with a simulated portfolio of `\\$ 100,000` in cash.\n",
    "- **Trading Positions**: The simulation permits positions to be either `1000 shares long` or `1000 shares short`. However, trading activity can involve up to `2000 shares` at a time, provided the net position does not exceed the constraints of being 1000 shares long or short.\n",
    "- **Benchmarking**: The performance of the trading strategies is measured against a benchmark scenario where `$100,000` is used to purchase `1000 shares of JPM` and held throughout the testing period. This benchmark serves to provide a comparative baseline for the performance of the developed strategies.\n",
    "\n",
    "### Transaction Costs\n",
    "\n",
    "- **Commissions and Market Impact**: Each trade in the simulation incorporates transaction costs, specifically a commission fee of `$9.95` per trade and a market impact cost of `0.005`. These costs are critical in mimicking real-world trading conditions and evaluating the net profitability of the strategies.\n",
    "\n",
    "This framework of market specifications and trading constraints is crucial for a realistic and rigorous assessment of the strategies. The implementation detail of these elements underscores my ability to develop a robust simulation environment that adheres closely to the operational realities of financial markets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c552f72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Market Simulator Overview\n",
    "\n",
    "### Functionality of the Market Simulator\n",
    "\n",
    "The market simulator I developed plays a pivotal role in evaluating the trading strategies implemented in this project. It is encapsulated within a Python function, `compute_portvals()`, which takes trading orders from a DataFrame and calculates the daily value of the trading portfolio over a specified period. This function returns a DataFrame with one column, representing the portfolio's total value on each trading day, indexed by date. This output facilitates a detailed performance analysis of the trading strategies under consideration.\n",
    "\n",
    "### Input and Output Specifications\n",
    "\n",
    "- **Input**: The function accepts a DataFrame `df_trades` which contains columns for the date of the trade, the symbol of the traded stock, the order type (BUY or SELL), and the number of shares traded.\n",
    "- **Output**: The resulting DataFrame, indexed by date, provides the total portfolio value for each day from the start date to the end date, inclusive. This value is calculated as the sum of the cash and the current value of all stock holdings.\n",
    "\n",
    "### Working of the Market Simulator\n",
    "\n",
    "The simulator keeps track of stock holdings and cash balance for each day, adjusting these according to the trades executed:\n",
    "- **Buying Stocks**: When a BUY order is placed, the simulator increases the stock count and deducts the cost (based on the adjusted closing price) from the cash balance.\n",
    "- **Selling Stocks**: Conversely, a SELL order decreases the stock count and adds the corresponding value to the cash balance.\n",
    "\n",
    "Negative shares indicate a short position, while negative cash signifies borrowing from the broker. The function ensures accurate portfolio valuation by updating these values daily based on the executed trades and market prices.\n",
    "\n",
    "### Code Implementation\n",
    "\n",
    "Below is the core implementation of the `compute_portvals()` function, tailored to handle the specified trading operations efficiently:\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_portvals(\n",
    "        df_trades,\n",
    "        symbol=\"JPM\",\n",
    "        start_val=100000,\n",
    "):\n",
    "    # Define the trading period\n",
    "    dates = pd.date_range(start=df_trades.index[0], end=df_trades.index[-1])\n",
    "    \n",
    "    # Initialize stock count and profit variables\n",
    "    symbol_count, profit = 0, 0\n",
    "    \n",
    "    # Fetch stock prices for the given dates and symbol\n",
    "    prices = get_data([symbol], dates)  # Automatically includes SPY for market comparison\n",
    "    prices = prices.drop(columns='SPY')  # Focus solely on the portfolio symbol\n",
    "    \n",
    "    # Initialize the DataFrame to store portfolio values\n",
    "    portfolio_values = pd.DataFrame(index=df_trades.index, columns=['Value'], data=np.nan)\n",
    "\n",
    "    # Iterate over each trading day to update portfolio values\n",
    "    for trade_day in df_trades.index:\n",
    "        share_price = prices.at[trade_day, symbol]\n",
    "        symbol_count += df_trades.at[trade_day, 'Trades']\n",
    "        profit += share_price * (-1) * df_trades.at[trade_day, 'Trades']\n",
    "        port_val = start_val + profit + symbol_count * share_price\n",
    "        portfolio_values.loc[trade_day, 'Value'] = port_val\n",
    "\n",
    "    return portfolio_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb06aac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypothetical Optimal Trading Strategy (TOS) Over the In-Sample Period\n",
    "\n",
    "#### Concept and Execution of TOS\n",
    "\n",
    "To understand the upper limits of potential returns under ideal conditions, a Hypothetical Optimal Trading Strategy (TOS) was developed for the JPM stock over the in-sample period from January 1, 2008, to December 31, 2009. This strategy involved making the most profitable trade possible on each day, assuming perfect foresight of the next dayâ€™s stock price movements. Essentially, this means buying, selling, or holding based on whether the stock price would increase, decrease, or stay the same the following day.\n",
    "\n",
    "#### Strategy Details\n",
    "\n",
    "- **Trading Actions**: On any given day, the optimal action (buy, sell, or hold) was determined based on the next day's price:\n",
    "  - **Buy**: If the next day's price was higher than the current day.\n",
    "  - **Sell**: If the next day's price was lower.\n",
    "  - **Hold**: If the next day's price was the same, or if the maximum amount of shares were already bought or sold in previous transactions.\n",
    "- **Constraints**: The strategy adhered to a limit of holding or shorting no more than 1000 shares at a time, and only one transaction type (buy or sell) was allowed per day.\n",
    "- **Performance**: The TOS yielded a cumulative return of 578.61% over the two-year period, showcasing what could theoretically be achieved with perfect market foresight.\n",
    "\n",
    "#### Mathematical and Practical Considerations\n",
    "\n",
    "The TOS provides an illustrative benchmark for the maximum possible returns under the given constraints but lacks practical applicability due to the unrealistic advantage of knowing future market movements. It serves as a theoretical upper boundary against which we can measure the performance of more realistic strategies like the manual and Q-Learning based strategies.\n",
    "\n",
    "### Code Revision for TOS Implementation\n",
    "\n",
    "Below is a revised version of the `testPolicy` function used to implement the TOS. This version enhances readability, incorporates best coding practices, and simplifies the control flow of the trading logic.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def testPolicy(symbol=\"JPM\", sd=pd.Timestamp('2008-01-01'), ed=pd.Timestamp('2009-12-31'), sv=10000):\n",
    "    dates = pd.date_range(start=sd, end=ed)\n",
    "    prices = get_data([symbol], dates).drop(columns='SPY')\n",
    "    df_trades = pd.DataFrame(0, index=prices.index, columns=['Trades'])\n",
    "    stock_count = 0\n",
    "    lot_size = 1000\n",
    "\n",
    "    for i in range(len(prices) - 1):\n",
    "        today_price = prices.iloc[i][symbol]\n",
    "        next_day_price = prices.iloc[i + 1][symbol]\n",
    "        action = np.sign(next_day_price - today_price) * lot_size\n",
    "\n",
    "        if stock_count + action > 1000 or stock_count + action < -1000:\n",
    "            action = 0  # Prevent exceeding holding constraints\n",
    "        df_trades.iloc[i] = action\n",
    "        stock_count += action\n",
    "\n",
    "    return df_trades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20549602",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Manual Trading Strategy\n",
    "\n",
    "### Overview of Technical Indicators\n",
    "\n",
    "The manual trading strategy in this project employs a set of technical indicators as key components for generating buy and sell signals. These indicators, namely Bollinger Bands Percentage (B%), Momentum, and Relative Strength Index (RSI), are selected for their ability to effectively gauge market conditions and guide trading decisions. The emphasis of this strategy is not on discovering a groundbreaking or overly complex approach, but rather on demonstrating my ability to implement a practical and effective strategy into code, which can then be evaluated against a machine learning-based approach.\n",
    "\n",
    "### Bollinger Bands Percentage (B%)\n",
    "\n",
    "Bollinger Bands are a statistical chart characterizing the prices and volatility over time of a financial instrument or commodity, using a formulaic method propounding standard deviations from a moving average. The Bollinger Bands % (B%) is calculated as follows:\n",
    "\n",
    "$ B\\% = \\left(\\frac{\\text{Price} - \\text{Lower Band}}{\\text{Upper Band} - \\text{Lower Band}} \\right) \\times 100 $\n",
    "\n",
    "where:\n",
    "- **Price** is the current closing price of the stock.\n",
    "- **Lower Band** is the SMA - 2 standard deviations.\n",
    "- **Upper Band** is the SMA + 2 standard deviations.\n",
    "- **SMA** is the Simple Moving Average over the last 20 days.\n",
    "\n",
    "**Signals**:\n",
    "- **Buy**: B% value near 0 indicates the price is at the lower band, suggesting an oversold market condition.\n",
    "- **Sell**: B% value near 100 indicates the price is at the upper band, suggesting an overbought condition.\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Momentum is a measure of the rate of change in stock prices, identifying the strength of price movements. It is calculated using the following equation:\n",
    "\n",
    "$\\text{Momentum} = \\left(\\frac{\\text{Current Price}}{\\text{Price of N days ago}} \\right) - 1$\n",
    "\n",
    "**Signals**:\n",
    "- **Buy**: A positive crossing above a specified threshold suggests that an upward price trend is likely to continue.\n",
    "- **Sell**: A negative crossing below a specified threshold suggests a continuing downward trend.\n",
    "\n",
    "### Relative Strength Index (RSI)\n",
    "\n",
    "The RSI is a momentum oscillator that measures the speed and change of price movements. It is calculated using the following formula:\n",
    "\n",
    "$\\text{RSI} = 100 - \\left( \\frac{100}{1 + \\frac{\\text{Average Gain}}{\\text{Average Loss}}} \\right)$\n",
    "\n",
    "where:\n",
    "- **Average Gain** and **Average Loss** are the average of the gains and losses over the last 14 days, respectively.\n",
    "\n",
    "**Signals**:\n",
    "- **Buy**: RSI below 30 suggests oversold conditions, potentially indicating an upcoming bullish reversal.\n",
    "- **Sell**: RSI above 70 indicates overbought conditions, suggesting a possible bearish reversal.\n",
    "\n",
    "### Strategy Implementation\n",
    "\n",
    "The implementation of this strategy in code involves a systematic application of these indicators to generate trading signals. This not only demonstrates the capability to translate financial analysis into actionable trading decisions but also sets the groundwork for a comparative evaluation with an AI-driven trading strategy. The goal is to showcase the practical application of these indicators within a structured trading strategy, emphasizing the ability to implement and test such strategies rigorously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700de71f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization and Parameter Tuning in the Manual Strategy\n",
    "\n",
    "### Overview of Optimization Process\n",
    "\n",
    "For the manual trading strategy, fine-tuning the parameters of the technical indicators is crucial for maximizing performance. The parameters for Bollinger Bands, Momentum, and RSI were systematically varied to identify the configuration that yields the best returns. This process involved exploring a range of values for each parameter and assessing their impact on the strategy's effectiveness.\n",
    "\n",
    "### Parameter Values Explored\n",
    "\n",
    "- bb_lookback_options = [30, 25, 20, 15]\n",
    "- bb_low_options = [5, 10, 15, 20]\n",
    "- bb_up_options = [85, 90, 95, 100]\n",
    "- mom_lookback_options = [4, 8, 12]\n",
    "- mom_low_options = [-10, -5, 0]\n",
    "- mom_up_options = [0, 5, 10, 15]\n",
    "- rsi_lookback_options = [4, 8, 12]\n",
    "- rsi_buy_up_options = [110, 105, 100, 90, 85]\n",
    "- rsi_buy_low_options = [10, 20, 30, 40]\n",
    "\n",
    "These values were chosen based on preliminary analysis, which suggested they encompass a reasonable range that could capture different market dynamics. This relative small range of parameters creates 138,240 possible parameter value combinations.\n",
    "\n",
    "### Utilizing Python's `concurrent.futures` for Parallel Execution\n",
    "\n",
    "To enhance the efficiency of the optimization process, I leveraged Python's `concurrent.futures` module, which simplifies the execution of function calls asynchronously. This module provides a high-level interface for asynchronously executing callables using pools of threads or processes. By using thread pools via `ThreadPoolExecutor` or process pools using `ProcessPoolExecutor`, it allows multiple function calls to be executed concurrently. This is particularly beneficial for computationally intensive tasks such as the parameter tuning phase in trading strategy optimization, where multiple configurations need to be tested in parallel to speed up the finding of an optimal set of parameters.\n",
    "\n",
    "### Code Implementation for Multi-threading Optimization\n",
    "\n",
    "Below is the code snippet from the project that demonstrates the use of `concurrent.futures` for optimizing the trading strategy parameters. This part of the code handles the submission of different parameter combinations to be processed in parallel, collecting the results, and identifying the combination that produces the highest portfolio value.\n",
    "\n",
    "Here is the improved code snippet with best practices applied, such as meaningful variable names, using a context manager for handling the process pool, and better documentation. The provided code is designed for clarity, maintainability, and efficiency, particularly in the context of Python's concurrency features.\n",
    "\n",
    "```python\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import itertools\n",
    "\n",
    "def optimize_strategy_parallel(symbol, start_date, end_date, fetch_prices, compute_portfolio_values, parameter_combinations):\n",
    "    \"\"\"\n",
    "    Optimizes trading strategy parameters in parallel using process pooling.\n",
    "\n",
    "    Args:\n",
    "        symbol (str): Stock symbol to optimize for.\n",
    "        start_date (datetime): Start date for data retrieval.\n",
    "        end_date (datetime): End date for data retrieval.\n",
    "        fetch_prices (callable): Function to fetch prices given a symbol and date range.\n",
    "        compute_portfolio_values (callable): Function to compute portfolio values given trades and prices.\n",
    "        parameter_combinations (list): List of parameter tuples to explore.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Best parameter set and its corresponding portfolio value.\n",
    "    \"\"\"\n",
    "    prices = fetch_prices(symbol=symbol, sd=start_date, ed=end_date)\n",
    "    best_portfolio_value = float('-inf')\n",
    "    best_parameters = None\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = [executor.submit(evaluate_parameters, params, prices) for params in parameter_combinations]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            portfolio_value, params = future.result()\n",
    "            if portfolio_value > best_portfolio_value:\n",
    "                best_portfolio_value = portfolio_value\n",
    "                best_parameters = params\n",
    "\n",
    "    return best_parameters, best_portfolio_value\n",
    "\n",
    "def evaluate_parameters(parameters, prices):\n",
    "    \"\"\"\n",
    "    Evaluates a single set of trading parameters.\n",
    "\n",
    "    Args:\n",
    "        parameters (tuple): Tuple containing parameters for Bollinger Bands, Momentum, and RSI.\n",
    "        prices (DataFrame): DataFrame containing price data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Last value of the portfolio and the parameters used.\n",
    "    \"\"\"\n",
    "    bb_lookback, bb_low, bb_up, mom_lookback, mom_low, mom_up, rsi_lookback, rsi_buy_up, rsi_buy_low = parameters\n",
    "    df_trades = apply_trading_strategy(prices, bb_lookback, bb_low, bb_up, mom_lookback, mom_low, mom_up, rsi_lookback, rsi_buy_up, rsi_buy_low)\n",
    "    port_vals = compute_portfolio_values(df_trades, prices)\n",
    "    return port_vals.iloc[-1], parameters  # Return the last value of the portfolio and the parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fbb168",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ML-Based Trading Strategy: Q-Learning and Dyna-Q\n",
    "\n",
    "#### Introduction to Q-Learning\n",
    "\n",
    "In this project, I apply a reinforcement learning strategy using Q-Learning to the same technical indicators used in the manual strategy, with the aim of teaching a machine to learn a policy for trading. Q-Learning is a model-free reinforcement learning algorithm that learns the value of an action in a particular state by using a Q-value, which is essentially a measure of the expected future rewards for a given action taken in a given state. It uses the Bellman Equation to iteratively update Q-values based on the equation:\n",
    "\n",
    "$Q(s, a) = Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$\n",
    "\n",
    "where:\n",
    "- $ s $  is the current state,\n",
    "- $  a $  is the current action,\n",
    "- $  r $  is the reward received after performing the action,\n",
    "- $  s' $  is the new state after the action,\n",
    "- $  \\alpha $  is the learning rate,\n",
    "- $  \\gamma $  is the discount factor,\n",
    "- $  \\max_{a'} Q(s', a') $  is the maximum predicted reward achievable in the new state.\n",
    "\n",
    "The Q-value for a state-action pair represents the expected future rewards that can be achieved by starting in that state, taking that action, and following the optimal policy thereafter. \n",
    "\n",
    "#### Dyna-Q Integration\n",
    "\n",
    "Dyna-Q integrates direct learning from experience (real interactions with the environment) and planning (simulations of the environment using a learned model). Each time a real action is taken and the agent transitions from state $ s$  to $ s'$ , receiving reward $  r $ , the model is updated. Then, the Dyna-Q algorithm revisits previous experiences, randomly samples them, and updates Q-values based on simulated experiences. This enhances learning efficiency by leveraging past experiences multiple times.\n",
    "\n",
    "#### Implementation Overview\n",
    "\n",
    "For this project, I implemented a Q-Learner from scratch which incorporates both Q-learning and Dyna-Q mechanisms. The key components of this Q-Learner include:\n",
    "\n",
    "- **Action Space**: Three possible actions - Buy, Sell, or Hold.\n",
    "- **State Space**: Discretized states based on multiple indicators such as price relative to moving averages, momentum, etc.\n",
    "- **Reward Function**: Designed to encourage profit maximization and cost minimization, including factors like transaction costs.\n",
    "- **Parameters**: Includes learning rate $ ( \\alpha )$ , discount factor $ ( \\gamma )$ , initial random action probability $ (rar)$ , and decay rate of randomness $ (radr)$ .\n",
    "\n",
    "### Code Implementation with Best Practices\n",
    "\n",
    "Below is the improved code snippet of the Q-Learner class, applying best coding practices for clarity and maintainability.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "class QLearner(object):\n",
    "    \"\"\"\n",
    "    Implementation of a Q-Learner with Dyna-Q functionality.\n",
    "    \n",
    "    Attributes:\n",
    "        num_states (int): Total number of states.\n",
    "        num_actions (int): Total number of actions.\n",
    "        alpha (float): Learning rate.\n",
    "        gamma (float): Discount factor.\n",
    "        rar (float): Random action rate.\n",
    "        radr (float): Decay rate of the random action rate.\n",
    "        dyna (int): Number of Dyna-Q simulated experience updates per real experience.\n",
    "        verbose (bool): Enables verbose mode for debugging.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_states=1728, num_actions=3, alpha=0.3, gamma=0.9,\n",
    "                 rar=0.65, radr=0.99, dyna=0, verbose=False):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.rar = rar\n",
    "        self.radr = radr\n",
    "        self.dyna = dyna\n",
    "        self.verbose = verbose\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "        self.model = {}  # Used for Dyna-Q\n",
    "\n",
    "    def querysetstate(self, s):\n",
    "        \"\"\"\n",
    "        Updates the state without updating the Q-table and decides action.\n",
    "        \n",
    "        Args:\n",
    "            s (int): The new state index.\n",
    "        \n",
    "        Returns:\n",
    "            int: The selected action index.\n",
    "        \"\"\"\n",
    "        self.s = s\n",
    "        action = rand.randint(0, self.num_actions - 1) if rand.random() <= self.rar else np.argmax(self.q_table[s])\n",
    "        if self.verbose:\n",
    "            print(f\"Querysetstate: s = {s}, a = {action}\")\n",
    "        return action\n",
    "\n",
    "    Below is a refined version of the `query` method within the Q-Learner class. This version is structured for clarity, with added comments to enhance understanding and readability. This method handles the updating of the Q-table based on the state transition and determining the next action to take. It also incorporates elements of the Dyna-Q approach to simulate learning from past experiences.\n",
    "\n",
    "\n",
    "    def query(self, s_prime, r):\n",
    "        \"\"\"\n",
    "        Updates the Q-table based on the transition to a new state and the reward received, then returns an action.\n",
    "    \n",
    "        Args:\n",
    "            s_prime (int): The new state index after taking an action.\n",
    "            r (float): The immediate reward received after transitioning to the new state.\n",
    "    \n",
    "        Returns:\n",
    "            int: The index of the selected action.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Update Q Table using the formula: Q(s, a) = (1 - alpha) * Q(s, a) + alpha * (reward + gamma * max(Q(s', all actions)))\n",
    "        a_prev = self.a\n",
    "        s_prev = self.s\n",
    "        max_future_reward = np.max(self.q_table[s_prime])\n",
    "        improved_estimate = (r + self.gamma * max_future_reward)\n",
    "\n",
    "        self.q_table[s_prev][a_prev] = (1 - self.alpha) * self.q_table[s_prev][a_prev] + self.alpha * improved_estimate\n",
    "\n",
    "        # Dyna-Q integration: Simulate learning from past experiences stored in the model\n",
    "        self.model[(self.s, self.a)] = (s_prime, r)\n",
    "        if self.dyna > 0 and len(self.model) > 0:\n",
    "        # Perform simulated updates from randomly sampled past experiences\n",
    "            sampled_keys = np.random.choice(list(self.model.keys()), min(self.dyna, len(self.model)), replace=True)\n",
    "            for s, a in sampled_keys:\n",
    "                s_prime, r = self.model[(s, a)]\n",
    "                max_future_reward = np.max(self.q_table[s_prime])\n",
    "                self.q_table[s][a] += self.alpha * (r + self.gamma * max_future_reward - self.q_table[s][a])\n",
    "\n",
    "        # 2) Determine action: either random based on the rar or the best possible based on the Q-table\n",
    "        if rand.uniform(0.0, 1.0) <= self.rar:\n",
    "            action = rand.randint(0, self.num_actions - 1)\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[s_prime])\n",
    "\n",
    "        # 3) Update state and action for the next step\n",
    "        self.s = s_prime\n",
    "        self.a = action\n",
    "        self.rar *= self.radr  # Update the probability of random action\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Updated Q-Table State: s = {s_prime}, a = {action}, r = {r}\")\n",
    "\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f16ab8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview of In-Sample Testing\n",
    "\n",
    "To assess the effectiveness of the Q-Learning based strategy and the manual strategy, we conducted a performance evaluation using historical data from the in-sample period, which spans from January 1, 2008, to December 31, 2009. The primary trading symbol for this test was JPM (JPMorgan Chase & Co.), with a starting portfolio value set at $\\$ 100,000$. Both strategies were subjected to standardized transaction costs, including a commission of $ \\$ 9.95$ per trade and a market impact of 0.005 per share traded.\n",
    "\n",
    "#### Experiment Setup\n",
    "\n",
    "- **Data Range**: January 1, 2008, to December 31, 2009.\n",
    "- **Trading Symbol**: JPM.\n",
    "- **Starting Portfolio Value**:  100,000.\n",
    "- **Transaction Costs**: Commission of  9.95 per trade and an impact of 0.005.\n",
    "- **Trading Conditions**: Both strategies could trade up to 1000 shares per transaction, with the capability to hold long, short, or neutral positions.\n",
    "\n",
    "The performance metrics analyzed included cumulative return, volatility (measured as the standard deviation of daily returns), and maximum drawdown over the period. These metrics help us understand not only the return on investment but also the risk involved in each strategy.\n",
    "\n",
    "#### Performance Outcomes and Initial Interpretations\n",
    "\n",
    "- **Manual Strategy**: Achieved a cumulative return of approximately 89%, indicating strong performance against the market conditions of the period.\n",
    "- **Strategy Learner (Q-Learner)**: Managed to achieve an even higher cumulative return of approximately 125%, showcasing the potential of machine learning in optimizing trading strategies beyond traditional methods.\n",
    "- **Benchmark Performance**: The benchmark, which involved buying and holding JPM stock from the first day, resulted in a cumulative return of around -8%. This highlights the superior performance of both the manual and AI-driven strategies over a simple buy-and-hold approach.\n",
    "\n",
    "![In-Sample Trading Strategy Performance](/images/perf_in.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a72f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Out-of-Sample Performance Evaluation\n",
    "\n",
    "#### Overview of Out-of-Sample Testing\n",
    "\n",
    "After tuning our strategies using in-sample data, we progress to evaluating their performance using completely unseen out-of-sample data. This test is crucial as it provides insights into how the strategies might perform under real market conditions, where the future is not known and cannot be optimized for in advance. For this analysis, the trading symbol remains JPM (JPMorgan Chase & Co.), and the period extends from January 1, 2010, to December 31, 2011.\n",
    "\n",
    "#### Performance Metrics\n",
    "\n",
    "- **Starting Portfolio Value**: \\$ 100,000\n",
    "- **Transaction Costs**: Standardized for both strategies with a commission of \\$ 9.95 per trade and an impact of 0.005 per share traded.\n",
    "- **Trading Conditions**: Up to 1000 shares were allowed per transaction, with positions being long, short, or neutral.\n",
    "\n",
    "The primary metrics for evaluation are cumulative return and volatility (standard deviation of daily returns). These indicators help us gauge not only the profitability of the strategies but also their risk profiles during the test period.\n",
    "\n",
    "#### Outcomes and Interpretation\n",
    "\n",
    "- **Manual Strategy**: Achieved a positive cumulative return of approximately 18% over the two-year out-of-sample period. This performance is significant as it demonstrates the strategy's effectiveness even under varied market conditions.\n",
    "- **Q-Learning Strategy**: Interestingly, the Q-Learner, which was trained using in-sample data, resulted in no trading activity for the first year of the out-of-sample period. This behavior might suggest that the policy learned was overly conservative or possibly too finely tuned to the in-sample data characteristics, leading to a hesitance or inability to trigger trades under slightly different market conditions. Additionally, it was not able to achieve a positive return over the out-sample period as seen below. \n",
    "\n",
    "![In-Sample Trading Strategy Performance](/images/perf_out.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7534827",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Conclusion: Insights and Forward-Looking Statements\n",
    "\n",
    "This project was an exploration into implementing and comparing different trading strategies with a focus on learning and adaptation rather than outright profitability. The manual strategy leveraged well-established financial indicators to navigate market conditions, while the ML-based strategy, utilizing Q-Learning, aimed to adapt and learn from evolving data patterns.\n",
    "\n",
    "The outcomes from these experiments highlighted several key points:\n",
    "- The manual strategy, though simple, proved effective in out-of-sample tests, reinforcing the value of traditional trading methods under certain conditions.\n",
    "- The Q-Learning approach, while promising in theory, faced practical challenges such as overfitting, underscoring the need for robust machine learning techniques that generalize well across different market scenarios.\n",
    "\n",
    "### Transitioning to Future Optimizations\n",
    "\n",
    "Given the findings and challenges observed, particularly with the Q-Learning strategy, there are several areas of potential optimization and research that could enhance the effectiveness and adaptability of trading algorithms:\n",
    "\n",
    "1. **Enhanced State Representation**: Integrating more complex state representations that capture a wider array of market dynamics could help in making more informed decisions.\n",
    "2. **Reward Function Enhancement**: To address the issue of the Q-Learner's inaction during crucial trading periods, refining the reward function could provide more dynamic feedback to the agent. Implementing a **risk-adjusted return measure**, such as the Sharpe ratio, as part of the reward function can encourage not just profit maximization but also optimal risk management. \n",
    "3. **Hybrid Models**: Combining Q-Learning with other machine learning approaches like deep neural networks could improve prediction accuracy and decision-making robustness.\n",
    "4. **Risk Management Integration**: Refining the reward structure to include risk management parameters would likely yield strategies that are not only profitable but also resilient.\n",
    "\n",
    "These optimizations could potentially bridge the gap between theoretical model performance and practical trading efficacy, leading to more robust trading strategies that can adapt to and capitalize on market complexities."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
